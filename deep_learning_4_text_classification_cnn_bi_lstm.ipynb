{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaUlHussna/PythonPractice/blob/main/deep_learning_4_text_classification_cnn_bi_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "cdae024e-9d51-46dd-bd28-70c7b4f91425",
        "_uuid": "84b70525e08f2a7d2ea6894583cebc8eb28d5972",
        "id": "CcY0Zl4MJyVf"
      },
      "cell_type": "markdown",
      "source": [
        "In this Kernel uses Manifestos data [reference here], and presents a pipeline of using Deep Learning Algorithms to classify manifesto topics [info about the types of domains]. In the current version the following types of Neural Network Algorithms have been implemented:\n",
        "* **Convolutional Neural Networks (CNN)** (Kim, 2014) with **Word2Vec** (Google) \n",
        "* **Long Short Term Memory (LSTM)** Recurrent Neural Networks, with **Word2Vec** (Google)\n",
        "\n",
        "## CNN & Word2Vec Implementation\n",
        "The general logic behind CNNs is presented in Kim (2014).  To use CNNs for sentence classification, imagine sentences and words as image pixels, where the input is sentences are represented as a matrix. \n",
        "\n",
        "Each row of the matrix is a vector that represents a sentence. \n",
        "\n",
        "This vector is the average of  **word2vec** (Google’s Word2Vec pre-trained model) scores of all words in our sentence.\n",
        "\n",
        "For 10 sentences using a 300-dimensional embedding we would have a 10×300 matrix as our input. \n",
        "That’s our “image”.\n",
        "\n",
        "For computational reasons, the number of steps/passes (epochs) has been set to 2 throughout. For improved accuracy set it to 20+ (I've seen it done up to 30), but you will need to run this on a separate server than kaggle (aws or locally with strong processors).\n"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "1e35fec1-f9a6-4d0d-9f56-a2abcc35d223",
        "_uuid": "beac69605322f4a98a30bd6cb7e33111e2a21242",
        "trusted": true,
        "id": "IXD71c5GJyVr"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, Flatten, Dropout, Add\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f52096ff6caa5405a264c436b52f33016c4c9ec",
        "id": "TSSNwfH6JyVu"
      },
      "cell_type": "markdown",
      "source": [
        "Note that we set the _num_epochs_ is low on purpose, so as not to exceed the 14GB RAM of training of the Word2Vec operation later on. For comparison purposes, it might actually be best to do at least 10, so that the histogram gives some more data points."
      ]
    },
    {
      "metadata": {
        "_cell_guid": "26d1feea-73fc-4438-b042-cbb58dcb6b3b",
        "_uuid": "d0ceacafb5f397e76044154734730acd93c68f15",
        "trusted": true,
        "id": "_Djwj_ixJyVv"
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 300 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n",
        "\n",
        "#training params\n",
        "batch_size = 256 \n",
        "num_epochs = 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f4edd7ec-7ddf-4c83-b3c5-f6dc751c82d4",
        "_uuid": "53086d9c9e086a9f66a4dc245dec759787b7bd7a",
        "trusted": true,
        "id": "xuodizqAJyVw"
      },
      "cell_type": "code",
      "source": [
        "train_comments = pd.read_csv(\"../input/manifestos-en/manifesots_en.csv\", sep=',', header=0)\n",
        "train_comments.columns=['text', 'cmp_code', 'eu_code', 'pos', 'manifesto_id', 'party', 'date', 'language', 'source', 'has_eu_code', 'is_primary_doc', 'may_contradict_core_dataset', 'md5sum_text', 'url_original', 'md5sum_original', 'annotations', 'handbook', 'is_copy_of', 'title', 'id']\n",
        "#'NA', '0', '101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708')\n",
        "print(\"num train: \", train_comments.shape[0])\n",
        "train_comments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "930cad5624c68cb53e3571f867d21fd1368d94e8",
        "id": "ySaWx3URJyVx"
      },
      "cell_type": "code",
      "source": [
        "# check that the values of cmp_code are of the right type\n",
        "print(train_comments.cmp_code[1])\n",
        "print(type(train_comments[\"cmp_code\"]))\n",
        "\n",
        "#turns values of cmp_code from object to a list, comma separated\n",
        "builder_list = [] #creates an empty list\n",
        "# loop - for every entry in the cmp_code column, add value to end of list, separated by commas\n",
        "for data in train_comments[\"cmp_code\"]: \n",
        "    builder_list.append(str(data))\n",
        "\",\".join(builder_list)\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# define dataset \n",
        "data = builder_list\n",
        "values = array(data)\n",
        "print(values[1])\n",
        "# encode codes to integer  \n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "# print the second entry as an integer code\n",
        "print(integer_encoded[1])\n",
        "# encode integer codes to binary\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "# print the second entry as a code expressed in binary\n",
        "print(onehot_encoded[1])\n",
        "\n",
        "# invert the vector to output the code. This throws an error of Deprecation (DeprecationWarning: The truth value \n",
        "# of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` \n",
        "# to check that an array is not empty.)- will need to go back to it to check.\n",
        "\n",
        "# print(type(onehot_encoded))\n",
        "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "# print(inverted[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "962c528c-a6a8-4b39-bc09-f83c3dc9a846",
        "_uuid": "f1e228fc842d11a46da231b337099d3756f43a1f",
        "trusted": true,
        "id": "bB98IFFnJyVy"
      },
      "cell_type": "code",
      "source": [
        "#label_names = ['101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708']\n",
        "#y_train = train_comments[label_names].values\n",
        "\n",
        "Y_train = onehot_encoded\n",
        "print(Y_train[1])\n",
        "\n",
        "# trying to add the vectors in the main dataset\n",
        "#clean_train_comments[\"codes_encoded\"] = clean_train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words]) \n",
        "#clean_train_comments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "89a34bbd-3df4-4843-9356-3ef48d5ecaba",
        "_uuid": "17ecdd78e8c098cd61e7ad19a5a1d39280983b17",
        "trusted": true,
        "id": "1Oyj933DJyVz"
      },
      "cell_type": "code",
      "source": [
        "#test_comments = pd.read_csv(\"../input/manifestos-aus/test-aus.csv\", engine='python', sep=',', header=0)\n",
        "#print(\"ok\")\n",
        "#test_comments.columns=['text', 'cmp_code', 'eu_code']\n",
        "#print(\"num test: \", test_comments.shape[0])\n",
        "#test_comments.head()\n",
        "\n",
        "#This is no longer needed, as I will create the test on the fly from the dataset above (manifesots_en.csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "72d3b1e6-9406-4961-b7b0-89c21cf37787",
        "_uuid": "d5dd53b853972acf59c5da3d8fee8812b415b263",
        "id": "_Ejp0pogJyV2"
      },
      "cell_type": "markdown",
      "source": [
        "**Cleaning Text**"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "3241b3ff-dfcf-4361-965d-6f7c71c8bfee",
        "_uuid": "d2d970eb5d9a4042ffa67b86d6d9732f45a31e68",
        "trusted": true,
        "id": "oON1eVvUJyV4"
      },
      "cell_type": "code",
      "source": [
        "def standardize_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
        "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
        "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
        "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
        "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "47da37eb-cc1d-40f5-8655-e2fdf6f36c97",
        "_uuid": "37157fd7ace420f22c708a616b0245740e8e499d",
        "trusted": true,
        "id": "pmjzq7OKJyV6"
      },
      "cell_type": "code",
      "source": [
        "train_comments.fillna('_NA_')\n",
        "train_comments.fillna('NaN')\n",
        "train_comments = standardize_text(train_comments, \"text\")\n",
        "train_comments.to_csv(\"train_clean_data.csv\")\n",
        "train_comments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "df5b86cf-a38a-4bb1-9a7c-95ef9d11f564",
        "_uuid": "992f605af050c9d395d42d76cdc6c89ddbd18f14",
        "trusted": true,
        "id": "dWQ9xH74JyV7"
      },
      "cell_type": "code",
      "source": [
        "#test_comments.fillna('_NA_')\n",
        "#test_comments = standardize_text(test_comments, \"text\")\n",
        "#test_comments.to_csv(\"test_clean_data.csv\")\n",
        "#test_comments.head()\n",
        "\n",
        "#This is no longer needed, as I will create the test on the fly from the dataset above (manifesots_en.csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "41b86444-27b9-476a-a767-b8d5cc3104de",
        "_uuid": "18749b4d4dac9e47a0f2c97e2ea7934889c50b02",
        "id": "_zG0HKSqJyV8"
      },
      "cell_type": "markdown",
      "source": [
        "**Tokenizing Text**"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "c2a2dea0-3008-4227-9032-d1d3442a1461",
        "_uuid": "5eb646e0533f485c39744bfe380b8ca605c3d843",
        "trusted": true,
        "id": "PVwqAGPPJyV8"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "clean_train_comments = pd.read_csv(\"train_clean_data.csv\")\n",
        "clean_train_comments['text'] = clean_train_comments['text'].astype('str') \n",
        "clean_train_comments.dtypes\n",
        "clean_train_comments[\"tokens\"] = clean_train_comments[\"text\"].apply(tokenizer.tokenize)\n",
        "# delete Stop Words\n",
        "clean_train_comments[\"tokens\"] = clean_train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n",
        "   \n",
        "clean_train_comments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "230ca18f-3a1e-4077-87ca-b77869f78acc",
        "_uuid": "64495855c6f06ecedfa4ea7537cbb80fe5671ce9",
        "trusted": true,
        "id": "HZzIjIlCJyV9"
      },
      "cell_type": "code",
      "source": [
        "#This is no longer needed, as I will create the test on the fly from the dataset above (manifesots_en.csv)\n",
        "#clean_test_comments = pd.read_csv(\"test_clean_data.csv\")\n",
        "#clean_test_comments['text'] = clean_test_comments['text'].astype('str') \n",
        "#clean_test_comments.dtypes\n",
        "#clean_test_comments[\"tokens\"] = clean_test_comments[\"text\"].apply(tokenizer.tokenize)\n",
        "#clean_test_comments[\"tokens\"] = clean_test_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n",
        "\n",
        "#clean_test_comments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8443ff4f-2bfc-43ab-8051-6d4543909e93",
        "_uuid": "5b788b67473bbfb913e35ed7ca5568559331e350",
        "trusted": true,
        "id": "DBVBXO-_JyV9"
      },
      "cell_type": "code",
      "source": [
        "all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
        "#print(clean_train_comments[\"tokens\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "21828cf4-cd50-4045-b779-e46fcd0c7d45",
        "_uuid": "5ea92372c340bfc6c4efd7086c026fe96290d5b9",
        "trusted": true,
        "id": "Rnoiqem_JyV-"
      },
      "cell_type": "code",
      "source": [
        "#all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
        "#test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
        "#TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "#print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "#print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5b053648-0414-4ba9-82c5-81342903806a",
        "_uuid": "c65f795647a86c1415839d6ed42654eb8f22ba9d",
        "id": "B4-gAwOEJyV-"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence. In this way we lose the syntax of our sentence, while keeping some semantic information.\n",
        "![](https://cdn-images-1.medium.com/max/1400/1*THo9NKchWkCAOILvs1eHuQ.png)"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "a871e0f7-2f49-4f9c-8d32-f0a183d7b439",
        "_uuid": "039e8adccac39202f6ee185407e39fec587ae1f5",
        "trusted": true,
        "id": "dCvyAS-tJyV-"
      },
      "cell_type": "code",
      "source": [
        "word2vec_path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\"\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c3fb56b5-6045-4995-bf5d-038cb67f467d",
        "_uuid": "f199f3c8fa270aab90400934f783712c1e6d2c55",
        "trusted": true,
        "id": "SoOy8fr4JyV_"
      },
      "cell_type": "code",
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, clean_train_comments, generate_missing=True)\n",
        "# test_embeddings = get_word2vec_embeddings(word2vec, clean_test_comments, generate_missing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "83fdccfb-9549-40d4-b929-787652a66062",
        "_uuid": "71eb13c4238975062db0a5d6f37f3273a3f43d14",
        "trusted": true,
        "id": "bzl884OQJyV_"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(clean_train_comments[\"text\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"text\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))\n",
        "\n",
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#print(train_cnn_data[:4])\n",
        "\n",
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights[1])\n",
        "print(\"-----------=====-----------\")\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a477520a-f8d2-4ab1-866c-a3a2cebbc7a7",
        "_uuid": "190b1764e182887b402bee3af641bbb6adb0c8b5",
        "trusted": true,
        "id": "Q7d0ZxYdJyWA"
      },
      "cell_type": "code",
      "source": [
        "#test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"text\"].tolist())\n",
        "#print(clean_test_comments[\"text\"][4])\n",
        "#print(test_sequences[4])\n",
        "#test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "515f99b8b3c3db6c0aad89d346dc8ca9fa577095",
        "id": "IjzQevNTJyWA"
      },
      "cell_type": "code",
      "source": [
        "#I opted for splitting the train set in two parts : a small fraction (20%) became the validation set which the model is evaluated and the rest (80%) is used to train the model.\n",
        "\n",
        "#Since our dataset is not balanced (explain) , a random split of the train set causes some labels to be over represented in the validation set and we end up with an unbalanced dataset. A simple random split could cause inaccurate evaluation during the validation, hence to avoid that, we use stratify = True option in train_test_split function (**Only for >=0.17 sklearn versions**).\n",
        "# Split dataset into training and test/validation\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Set the random seed\n",
        "random_seed = 2\n",
        "\n",
        "Y_train = onehot_encoded #(defined above)\n",
        "#X_train = clean_train_comments[\"tokens\"]\n",
        "X_train = train_embedding_weights\n",
        "print(X_train[0])\n",
        "print(Y_train[0])\n",
        "print(clean_train_comments[\"cmp_code\"][0])\n",
        "print(clean_train_comments[\"text\"][0])\n",
        "print(clean_train_comments[\"tokens\"])\n",
        "\n",
        "print(\"num X_train: \", X_train.shape[0])\n",
        "print(\"num Y_train: \", Y_train.shape[0])\n",
        "# Split the train and the validation set for the fitting\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state=random_seed)\n",
        "\n",
        "#print(X_val)\n",
        "#print(Y_val)\n",
        "print(\"num start dataset: \", clean_train_comments.shape[0])\n",
        "print(\"num X_val: \", X_val.shape[0])\n",
        "print(\"num Y_val: \", Y_val.shape[0])\n",
        "print(\"num X_train: \", X_train.shape[0])\n",
        "print(\"num Y_train: \", Y_train.shape[0])\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9d256e6f-aab4-44ac-953b-be50b586be55",
        "_uuid": "45f5ba479f261f3a05e06f69b78183a46819b953",
        "id": "3jWtIkJrJyWB"
      },
      "cell_type": "markdown",
      "source": [
        "Define a Convolutional Neural Network following Yoon Kim model [2]"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "54b65983-a6ed-4509-921e-c459840a1040",
        "_uuid": "8f89ca5fc4fa41bc002cbac0738dc56cb6d7aca1",
        "trusted": true,
        "id": "X7C_vzYUJyWB"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers.merge import concatenate, add\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
        "    #the filter\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable)\n",
        "\n",
        "    #the unknown image\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    #the merge function of the first convolution \n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    convs = []\n",
        "    filter_sizes = [3,4,5] # in the loop, first apply 3 as size, then 4 then 5\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        #kernel is the filter\n",
        "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    \n",
        "    # activated if extra_convoluted is true at the def\n",
        "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    pool = MaxPooling1D(pool_size=3)(conv)\n",
        "\n",
        "    if extra_conv==True:\n",
        "        x = Dropout(0.5)(l_merge)  \n",
        "    else:\n",
        "        # Original Yoon Kim model\n",
        "        x = Dropout(0.5)(pool)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # Finally, we feed the output into a Sigmoid layer.\n",
        "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
        "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f32f31b8-eda6-4f59-90ec-14eb9d4a9712",
        "_uuid": "1bbb0e66fc5ad6ef4521a9d400db3127969070c6",
        "trusted": true,
        "id": "03oDCcxsJyWC"
      },
      "cell_type": "code",
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train\n",
        "print(len(list(label_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "23c15b32-ef1f-4c6e-8977-161f948470fe",
        "_uuid": "65613319e466f72cda5a1d27a35aae620fb040ad",
        "trusted": true,
        "id": "f54hyAWEJyWC"
      },
      "cell_type": "code",
      "source": [
        "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)), False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3f3b7c3c-cecd-41bc-a62a-cb22ddbebce6",
        "_uuid": "843d512895d45ced0a2f9463f4fbabdccacd6d7b",
        "trusted": true,
        "id": "xRyDYVH-JyWC"
      },
      "cell_type": "code",
      "source": [
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20f5d7519b9214182bfedabe58841b412f054791",
        "id": "MBE8qwH-JyWD"
      },
      "cell_type": "code",
      "source": [
        "# I opted for splitting the train set in two parts : a small fraction (20%) became the validation set which the model is \n",
        "# evaluated and the rest (80%) is used to train the model.\n",
        "# Since our dataset is not balanced (explain) , a random split of the train set causes some labels to be over represented \n",
        "# in the validation set and we end up with an unbalanced dataset. A simple random split could cause inaccurate evaluation \n",
        "# during the validation, hence to avoid that, we use stratify = True option in train_test_split function \n",
        "# (**Only for >=0.17 sklearn versions**).\n",
        "\n",
        "\n",
        "\n",
        "# Split dataset into training and test/validation\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Set the random seed\n",
        "random_seed = 2\n",
        "\n",
        "Y_train = onehot_encoded #(defined above)\n",
        "X_train = clean_train_comments[\"tokens\"]\n",
        "print(X_train[0])\n",
        "print(Y_train[0])\n",
        "print(clean_train_comments[\"cmp_code\"][0])\n",
        "print(clean_train_comments[\"text\"][0])\n",
        "\n",
        "print(\"num X_train: \", X_train.shape[0])\n",
        "print(\"num Y_train: \", Y_train.shape[0])\n",
        "# Split the train and the validation set for the fitting\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state=random_seed) # can also use random_state=random_seed as an option if dataset is balanced\n",
        "\n",
        "#print(X_val)\n",
        "#print(Y_val)\n",
        "print(\"num start dataset: \", clean_train_comments.shape[0])\n",
        "print(\"num X_val: \", X_val.shape[0])\n",
        "print(\"num Y_val: \", Y_val.shape[0])\n",
        "print(\"num X_train: \", X_train.shape[0])\n",
        "print(\"num Y_train: \", Y_train.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2d8225c5-4cc6-4c52-824b-5217e62c7c29",
        "_uuid": "fc16b6d9009948108f5227b0ccf0296edd074bfc",
        "id": "UdGd2535JyWD"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's train our Neural Network"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "3d6d73bd-d9e8-494a-88d7-04295a030e3c",
        "_uuid": "813df22e089125425c34e1e73ed039c0471ef9c6",
        "trusted": true,
        "id": "PHjW_Vz1JyWD"
      },
      "cell_type": "code",
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "984173d7-b21d-4a4d-b613-35f609f13853",
        "_uuid": "562cbcd921cdf6c3d575eb6350e5918e83cca81d",
        "trusted": true,
        "id": "B0vQAPczJyWE"
      },
      "cell_type": "code",
      "source": [
        "y_test = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "65022913-00b5-4bb8-a4f6-71fc30406afb",
        "_uuid": "0d1849e93ae0ff6acdc7744c1de2814c6c702f6c",
        "trusted": true,
        "id": "emP2hpewJyWE"
      },
      "cell_type": "code",
      "source": [
        "#create a submission\n",
        "submission_df = pd.DataFrame(columns=['id'] + label_names)\n",
        "submission_df['id'] = test_comments['id'].values \n",
        "submission_df[label_names] = y_test \n",
        "submission_df.to_csv(\"./cnn_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4ef1bc6c-6d46-4d7f-8900-b833a8165e09",
        "_uuid": "8e092255d6d67075be858bfe1f3b3dc731cf1653",
        "trusted": true,
        "id": "PbK2ok1DJyWF"
      },
      "cell_type": "code",
      "source": [
        "#generate plots\n",
        "plt.figure()\n",
        "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
        "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "plt.title('CNN sentiment')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c88d722c-1921-41ce-8a8c-ab8efcc4e7a2",
        "_uuid": "edf14e826df5db0f231459b9a3a60624b7f31463",
        "trusted": true,
        "id": "cZ8nDr-OJyWG"
      },
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
        "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
        "plt.title('CNN sentiment')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a084ea8c47ac7248f7fee0c85dd880d08bc868bb",
        "id": "k-kzjBdIJyWH"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM (bidirectional RNN) & Word2Vec\n",
        "Using the trained word to vector datasets, this section will classify the test sentences using a type of Recurrent Neural Network (Long Short Term Model) and Word2Vec, using Keras libraries."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9019680f810afe18ac34d26c6451efc8d7a72f92",
        "id": "zdsfujEPJyWI"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "710ca219ac8951f724141bccd14af39d99b3844c",
        "id": "uwtaasglJyWJ"
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz'\n",
        "train = pd.read_csv('../input/manifestos-aus/train-aus.csv')\n",
        "test = pd.read_csv('../input/manifestos-aus/test-aus.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01f28d7e9033428010d9d5b7718a2c477233fa5a",
        "id": "DiareRa2JyWJ"
      },
      "cell_type": "code",
      "source": [
        "train[\"text\"].fillna(\"fillna\")\n",
        "test[\"text\"].fillna(\"fillna\")\n",
        "X_train = train[\"text\"].str.lower()\n",
        "y_train = train[['101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708']].values\n",
        "\n",
        "X_test = test[\"text\"].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1881bb6cf4e07fcc988911adff519174218c26cd",
        "id": "qXCBi50jJyWJ"
      },
      "cell_type": "code",
      "source": [
        "max_features=100000\n",
        "maxlen=150\n",
        "embed_size=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4dd30c550f6e430708b4204cbbfc7a42a2ebb9e",
        "id": "EpavF5-dJyWK"
      },
      "cell_type": "code",
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ab735aaad3ca3e0f2e4a94d7017b204a5026bcf",
        "id": "W0fygOUSJyWK"
      },
      "cell_type": "code",
      "source": [
        "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
        "tok.fit_on_texts(list(X_train)+list(X_test))\n",
        "X_train=tok.texts_to_sequences(X_train)\n",
        "X_test=tok.texts_to_sequences(X_test)\n",
        "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
        "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "693890fa89e553126a522ffe25ebdd4508e171f3",
        "id": "tWjLTjEoJyWK"
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb0dc747b4845f29258a5c32aa30887407a017bf",
        "id": "MkiwmW4YJyWK"
      },
      "cell_type": "code",
      "source": [
        "word_index = tok.word_index\n",
        "#prepare embedding matrix\n",
        "num_words = min(max_features, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "026fb4c0258e2702c2bff765f99a2c6f5fffec1b",
        "id": "qoTaEp7-JyWL"
      },
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(maxlen, ))\n",
        "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
        "x = SpatialDropout1D(0.2)(x)\n",
        "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
        "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "x = concatenate([avg_pool, max_pool]) \n",
        "# x = Dense(128, activation='relu')(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "preds = Dense(6, activation=\"sigmoid\")(x)\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ccf8744eadf4723f92b9cb5da9cee4d0b565cd14",
        "id": "vbGibszzJyWL"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 4\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17f4f3d277e3ab49fad27b0b54b89753db9a8643",
        "id": "CAMI-P5wJyWL"
      },
      "cell_type": "code",
      "source": [
        "# filepath=\"../input/best-model/best.hdf5\"\n",
        "#filepath=\"weights_base.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
        "ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
        "callbacks_list = [ra_val,checkpoint, early]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32580b9fbc949e6b669674390cf23a1ad5fdb252",
        "id": "U78DL52zJyWL"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
        "#Loading model weights\n",
        "#model.load_weights(filepath) #try this with and without; with is bi-LSTM with convolution\n",
        "print('Predicting....')\n",
        "y_pred = model.predict(x_test,batch_size=1024,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ba5f78347be6b7699405d84ec3b7b2458d31ece",
        "id": "bQiW9TvnJyWM"
      },
      "cell_type": "code",
      "source": [
        "# Write scores to file\n",
        "submission = pd.read_csv('../input/manifestos-aus/sample_submission.csv')\n",
        "submission[['101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708']] = y_pred\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dbca83d0-aa29-457f-afff-a7e13b19ddef",
        "_uuid": "ed1776d31f77de38e76530bf190102e271c4117f",
        "id": "ijEUE4u9JyWM"
      },
      "cell_type": "markdown",
      "source": [
        "**References**:   \n",
        "* [1] How to solve 90% of NLP problems: a step-by-step guide\n",
        " * https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n",
        "* [2] Yoon Kim model\n",
        " * https://arxiv.org/abs/1408.5882\n",
        "* [3] Understanding Convolutional Neural Networks for NLP:\n",
        " * http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "deep-learning-4-text-classification-cnn-bi-lstm.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}